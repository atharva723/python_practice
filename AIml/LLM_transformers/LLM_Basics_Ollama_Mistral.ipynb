{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58c90cc9",
   "metadata": {},
   "source": [
    "# Basics of LLMs with Ollama + Mistral\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "faf43c39",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Environment Setup\n",
    "py -3.11 -m venv ollama-env\n",
    "ollama-env\\Scripts\\activate.bat\n",
    "python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78ece7ac",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2671665110.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    python -m ipykernel install --user --name=ollama-env --display-name \"Python (ollama-env)\"\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "!pip install ipykernel\n",
    "python -m ipykernel install --user --name=ollama-env --display-name \"Python (ollama-env)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "626856e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\User\\\\.conda\\\\envs\\\\training_env\\\\python.exe'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dafeb8",
   "metadata": {},
   "source": [
    "## 2. Hello LLM â€” Single Prompt (Generate API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df001abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello! I'm a model of an AI assistant designed to help answer questions, provide information, assist in tasks, and engage in conversation. I can help with things like setting reminders, providing weather updates, answering trivia questions, helping with math problems, and much more. I can also help you manage your schedule, book appointments, and even control smart home devices if they are connected to the internet. Let me know how I can assist you today!\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.generate(\n",
    "    model='mistral',\n",
    "    prompt=\"Hello! What can you do?\"\n",
    ")\n",
    "print(response['response'])\n",
    "\n",
    "# OpenAI Equivalent\n",
    "# from openai import OpenAI\n",
    "# client = OpenAI()\n",
    "# client.responses.create(model=\"gpt-4.1-mini\", input=\"Hello! What can you do?\")\n",
    "\n",
    "# Hugging Face Equivalent (commented)\n",
    "# from transformers import pipeline\n",
    "# nlp = pipeline(\"text-generation\", model=\"mistral\")\n",
    "# nlp(\"Hello! What can you do?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84306964",
   "metadata": {},
   "source": [
    "## 3. Conversation â€” Chat API (with Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f80be0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mysuru, also known as Mysore, is a historic city located in the southwestern state of Karnataka, India. It's renowned for its rich history, vibrant culture, and beautiful architecture. The city is particularly famous for Mysore Palace, an imposing edifice with a striking combination of Hindu, Saracenic, and Rajput architectural styles.\n",
      "\n",
      "Mysuru also hosts the Mysore Dussehra festival, one of India's grandest and most famous cultural events celebrated every year. The city is known for its silk industry, producing some of the finest quality silk sarees in the world.\n",
      "\n",
      "Another notable attraction is the Chamundi Hills, where the Chamundeshwari Temple dedicated to the goddess Chamundeshwari, the patron deity of Mysuru, stands atop. The city is also home to several educational institutions of national importance such as the University of Mysore and the Indian Institute of Science Education and Research (IISER).\n",
      "\n",
      "Mysuru's pleasant climate, green surroundings, and rich cultural heritage make it a popular tourist destination in India.\n",
      " Mysuru is located in the southern part of India, in the state of Karnataka. It lies approximately 145 kilometers (90 miles) southwest of Bangalore, which is the capital city of Karnataka. Mysuru is situated at an elevation of about 768 meters (2,520 feet) above sea level in a region known as the Mysore Plateau or the Deccan Plateau. The city's geographical coordinates are:\n",
      "\n",
      "Latitude: 12.3958Â° N\n",
      "Longitude: 76.5426Â° E\n",
      "\n",
      "These coordinates can be used to find the exact location of Mysuru on maps and navigation apps.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me something about Mysuru.\"}\n",
    "]\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='mistral',\n",
    "    messages=messages\n",
    ")\n",
    "print(response['message']['content'])\n",
    "\n",
    "# Memory is retained by sending back the full messages list!\n",
    "messages.append(response['message'])\n",
    "\n",
    "# Follow-up query\n",
    "messages.append({\"role\": \"user\", \"content\": \"Where is that located?\"})\n",
    "response = ollama.chat(model='mistral', messages=messages)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "47c13882",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "openAI Equivalent\n",
    "client.chat.completions.create(\n",
    "  model=\"gpt-4.1-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a historian.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain Mysuru Palace\"}\n",
    "  ]\n",
    ")\n",
    "Hugging Face Equivalent\n",
    "from huggingface_hub import InferenceClient\n",
    "client = InferenceClient(model=\"mistralai/Mistral-7B-Instruct\")\n",
    "\n",
    "client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a historian.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain Mysuru Palace\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10d93af4",
   "metadata": {},
   "source": [
    "# âœ… Ollama Python API â€” Two Different Interfaces\n",
    "\n",
    "Ollama provides two ways to call models:\n",
    "\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "1ï¸âƒ£ Simple API (Top-Level Functions)\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "âœ… Best for: Demos, learning, testing\n",
    "âœ… Automatic connection to localhost\n",
    "âœ… Minimal code\n",
    "\n",
    "    import ollama\n",
    "    response = ollama.generate(model=\"mistral\", prompt=\"Hello\")\n",
    "\n",
    "When to use:\n",
    "â€¢ Quick single-turn prompts\n",
    "â€¢ Exploring model responses\n",
    "â€¢ Teaching beginner LLM concepts\n",
    "\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "2ï¸âƒ£ Client API (Explicit Client)\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "âœ… Best for: Applications and production\n",
    "âœ… More control: host, retries, streaming, multi-turn chat\n",
    "\n",
    "    from ollama import Client\n",
    "    client = Client(host=\"http://localhost:11434\")\n",
    "    response = client.chat(model=\"mistral\", messages=[...])\n",
    "\n",
    "When to use:\n",
    "â€¢ Multi-turn chat apps\n",
    "â€¢ Remote Ollama servers (LAN / Docker)\n",
    "â€¢ Streaming token responses\n",
    "â€¢ Advanced settings and control\n",
    "\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ðŸ’¡ Summary:\n",
    "â€¢ Start learning with the simple API\n",
    "â€¢ Use Client API when building real chatbots / agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39e77f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Embeddings are a way of representing data points from various domains, such as text or images, as vectors in a lower-dimensional space while preserving their semantic meaning.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.generate(\n",
    "    model=\"mistral\",\n",
    "    prompt=\"Explain what embeddings are in 1 sentence.\"\n",
    ")\n",
    "\n",
    "print(response[\"response\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "037e737c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Who was the last king of Mysuru?\n",
      "Model:  The last Maharaja (King) of Mysore, as it is officially known today, was Sri Jayachamarajendra Wodeyar. He reigned from 1947 to 1949. However, it's important to note that the princely state of Mysore was integrated into the Indian Union in 1947 and Jayachamarajendra Wodeyar served as the Rajpramukh (Governor) of Mysore State from 1947 until 1952, when India became a republic and the position of Rajpramukh was abolished. He abdicated the throne in 1949.\n",
      "\n",
      "User: Where was he born?\n",
      "Model:  To provide an accurate answer, I would need a specific name or identity. If you're referring to a historical figure, public personality, or fictional character, please specify who you mean, and I'll be happy to help!\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "# Try multi-turn chat\n",
    "print(\"User: Who was the last king of Mysuru?\")\n",
    "res1 = ollama.generate(model=\"mistral\", prompt=\"Who was the last king of Mysuru?\")\n",
    "print(\"Model:\", res1[\"response\"])\n",
    "\n",
    "print(\"\\nUser: Where was he born?\")\n",
    "res2 = ollama.generate(model=\"mistral\", prompt=\"Where was he born?\")\n",
    "print(\"Model:\", res2[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464c23fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "\n",
    "client = Client()  # auto host = http://localhost:11434\n",
    "\n",
    "messages = []\n",
    "\n",
    "def chat(user_message):\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "    response = client.chat(model=\"mistral\", messages=messages)\n",
    "    messages.append(response[\"message\"])\n",
    "    print(\"Assistant:\", response[\"message\"][\"content\"])\n",
    "\n",
    "chat(\"Who was the last king of Mysuru?\")\n",
    "chat(\"Where was he born?\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c122ad43",
   "metadata": {},
   "source": [
    "LLMs do not have actual memory.\n",
    "We maintain memory by:\n",
    "â€¢ Storing conversation history in the application (Python list)\n",
    "â€¢ Sending full history again with every new request\n",
    "\n",
    "LLM memory is NOT inside the model â†’ It is in the APP layer.\n",
    "\n",
    "Later: LangChain helps automate this (Memory, Buffering, Summarization)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "69f4a849-e41b-4af9-9422-2ca4fb70e247",
   "metadata": {},
   "source": [
    " Jab aapka chatbot lambi baatein karne lagta hai, toh aapki `messages` list (memory) bharti jati hai. Isse do bade nuksan hote hain:\n",
    "\n",
    "1. **RAM Full:** Aapke 32GB RAM mein model toh fit hai, lekin bahut lambi history tokens consume karti hai.\n",
    "2. **Context Window Limit:** Har model ki ek limit hoti hai (jaise Llama 3 ki 8k ya 128k tokens). Agar history isse badi ho gayi, toh model purani baatein bhulne lagega ya error de dega.\n",
    "\n",
    "Ise manage karne ke **3 bade tarike** hain:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Sliding Window (The \"Sabse Purana Bhool Jao\" Rule)\n",
    "\n",
    "Isme hum sirf pichle 5 ya 10 messages rakhte hain. Jaise hi naya message aata hai, hum sabse purana message delete kar dete hain.\n",
    "\n",
    "```python\n",
    "# Sirf pichle 5 messages yaad rakho (System prompt ko chhod kar)\n",
    "if len(messages) > 6: \n",
    "    messages = [messages[0]] + messages[-5:] \n",
    "\n",
    "```\n",
    "\n",
    "* **Fayda:** RAM kabhi nahi bharegi.\n",
    "* **Nuksan:** Agar 20 messages pehle koi important baat hui thi, toh AI use bhool jayega.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Summarization (The \"Short Mein Batao\" Rule)\n",
    "\n",
    "Jab history lambi ho jaye, toh hum AI se kehte hain ki: *\"Ab tak ki saari baaton ka ek short summary bana do.\"* Phir hum saari purani messages hata kar sirf wo **Summary** aur **System Prompt** rakhte hain.\n",
    "\n",
    "```python\n",
    "# Fake logic: AI ko summary banane ko bolna\n",
    "summary = ollama.generate(model='mistral', prompt=f\"Summarize this: {messages}\")\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": f\"History summary: {summary['response']}\"},\n",
    "    {\"role\": \"user\", \"content\": \"Next question...\"}\n",
    "]\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. RAG (The \"Library\" Method) - **Best for Professionals**\n",
    "\n",
    "Agar aap chahte hain ki AI ko 1 saal purani baat bhi yaad rahe, toh hum **Vector Database** use karte hain.\n",
    "\n",
    "* Puri history ko ek file/database mein save kar diya jata hai.\n",
    "* Jab aap kuch puchte hain, toh code search karta hai ki history mein isse milti-julti baat kahan hui thi.\n",
    "* Sirf wahi \"relevant\" part nikaal kar model ko bheja jata hai."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5d8662",
   "metadata": {},
   "source": [
    "## 4. Tokens Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50bb7393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here is how the sentence \"Who was the last king of Mysuru?\" can be tokenized:\n",
      "\n",
      "1. Who\n",
      "2. was\n",
      "3. the\n",
      "4. last\n",
      "5. king\n",
      "6. of\n",
      "7. Mysuru?\n",
      "\n",
      "Each word or phrase (except for contractions) is a token in this context.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Who was the last king of Mysuru?\"\n",
    "response = ollama.generate(model=\"mistral\", \n",
    "                           prompt=f\"Tokenize this: {prompt}\")\n",
    "print(response['response'])\n",
    "\n",
    "\n",
    "# Tokens = cost + context length!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e63439",
   "metadata": {},
   "source": [
    "## 5. Embeddings (Meaning Understanding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f91ebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc6f7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def embed(text):\n",
    "    return ollama.embeddings(model=\"mistral\", prompt=text)['embedding']\n",
    "\n",
    "v1 = embed(\"king\")\n",
    "v2 = embed(\"queen\")\n",
    "\n",
    "similarity = np.dot(v1, v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "print(\"Similarity:\", similarity)\n",
    "\n",
    "# HF Equivalent (commented)\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# sim = util.cos_sim(model.encode(\"king\"), model.encode(\"queen\"))\n",
    "# print(sim)\n",
    "print(v1)\n",
    "print(v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1632fcea",
   "metadata": {},
   "source": [
    "## 6. Temperature (Creativity Control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b281950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp 0.0:  In the heart of Karnataka, where royalty whispers in every breeze, lies Mysuru, a city that dances between tradition and modernity with graceful elegance.\n",
      "Temp 0.7:  In Mysuru, where the sun paints silk-city mornings with gold hues and jasmine whispers dance in the air, every moment echoes a symphony of timeless charm.\n",
      "Temp 1.2:  \"Mysore, where the whispering silks of Mysore Palace twirl in rhythm with the lilting melodies of a traditional Yakshagana performance, a vibrant dance of history and culture.\"\n",
      "\n",
      "The city of Mysuru, renowned for its opulent palaces and rich cultural heritage, is known for many things. The famous Mysore Palace is one such landmark that reflects the grandeur and elegance of the region's history. At the same time, Yakshagana, a traditional form of theater unique to Karnataka, showcases the vibrant and vivacious spirit of the city and its people. This creative line aims to encapsulate both the historical beauty and the lively cultural essence that Mysore embodies.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "q = \"Write a creative line about Mysuru.\"\n",
    "for t in [0.0, 0.7, 1.2]:\n",
    "    r = ollama.chat(model=\"mistral\",\n",
    "                    messages=[{\"role\":\"user\",\"content\":q}],\n",
    "                    options={'temperature':t})\n",
    "    print(f\"Temp {t}: {r['message']['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2477c467",
   "metadata": {},
   "source": [
    "## 7. Streaming Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c99a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = ollama.chat(\n",
    "    model=\"mistral\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me a short story about a tiger.\"}],\n",
    "    stream=True,\n",
    ")\n",
    "for chunk in stream:\n",
    "    print(chunk['message']['content'], end=\"\", flush=True)\n",
    "\n",
    "# OpenAI Equivalent\n",
    "# client.responses.create(..., stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6c30cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Transformers are a type of machine learning model used for natural language processing tasks, such as translating from one language to another or generating text. They were introduced by Google researchers in 2017 and have since become one of the most popular models for NLP tasks due to their ability to capture long-range dependencies in text.\n",
      "\n",
      "2. Transformers consist of a series of self-attention layers, which allow each word in the input sequence to attend to every other word in the sequence, weighted by the relevance of each word for understanding the current word's meaning. This allows the model to focus on relevant information and ignore irrelevant details, resulting in more accurate predictions.\n",
      "\n",
      "3. Unlike recurrent neural networks (RNNs) and convolutional neural networks (CNNs), transformers do not use recurrence or convolution, making them faster and easier to train on long sequences. Transformers also handle parallelization better than RNNs and CNNs, allowing for more efficient training on large datasets."
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "stream = client.generate(\n",
    "    model=\"mistral\",\n",
    "    prompt=\"Explain transformers in 3 short bullets.\",\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk[\"response\"], end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfe614a",
   "metadata": {},
   "source": [
    "## 8. Context Window Limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd11e4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Mysuru \" * 1500\n",
    "response = ollama.chat(\n",
    "    model=\"mistral\",\n",
    "    messages=[{\"role\":\"user\", \"content\":text + \"\\nWhat word did I repeat?\"}]\n",
    ")\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f152e0",
   "metadata": {},
   "source": [
    "## How Chat Memory Works\n",
    "We manually maintain full conversation in `messages` list\n",
    "\n",
    "âž¡ Ollama does NOT store memory internally  \n",
    "âž¡ We send all history every time (just like OpenAI)\n",
    "\n",
    "This leads to **context window limits** â€” motivation for RAG + LangChain Memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f61923",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Next: LangChain\n",
    "Before jumping, remember:\n",
    "| Feature | Raw Ollama | With LangChain |\n",
    "|--------|------------|----------------|\n",
    "| Memory | Manual messages[] | Builtâ€‘in memory modules |\n",
    "| Tools & Agents | Hard to implement | 1â€‘line setup |\n",
    "| RAG | Manual vector DB | Automated pipelines |\n",
    "| Workflows | Custom code | Chains & Executors |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

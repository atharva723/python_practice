{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58c90cc9",
   "metadata": {},
   "source": [
    "# Basics of LLMs with Ollama + Mistral\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "faf43c39",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Environment Setup\n",
    "py -3.11 -m venv ollama-env\n",
    "ollama-env\\Scripts\\activate.bat\n",
    "python --version"
   ]
  },
  {
   "cell_type": "raw",
   "id": "78ece7ac",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "pip install ipykernel\n",
    "python -m ipykernel install --user --name=ollama-env --display-name \"Python (ollama-env)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "626856e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\LTI\\\\ollama-env\\\\Scripts\\\\python.exe'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47618f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ollama jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dafeb8",
   "metadata": {},
   "source": [
    "## 2. Hello LLM â€” Single Prompt (Generate API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df001abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello! I'm here to help answer questions, provide information, generate ideas, and engage in meaningful conversations on a wide range of topics. I can assist with math problems, explain complex concepts, help brainstorm creative solutions, and even share jokes or fun facts. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.generate(\n",
    "    model='mistral',\n",
    "    prompt=\"Hello! What can you do?\"\n",
    ")\n",
    "print(response['response'])\n",
    "\n",
    "# OpenAI Equivalent\n",
    "# from openai import OpenAI\n",
    "# client = OpenAI()\n",
    "# client.responses.create(model=\"gpt-4.1-mini\", input=\"Hello! What can you do?\")\n",
    "\n",
    "# Hugging Face Equivalent (commented)\n",
    "# from transformers import pipeline\n",
    "# nlp = pipeline(\"text-generation\", model=\"mistral\")\n",
    "# nlp(\"Hello! What can you do?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84306964",
   "metadata": {},
   "source": [
    "## 3. Conversation â€” Chat API (with Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f80be0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mysuru, also known as Mysore, is a historic city located in the southwestern state of Karnataka, India. It's renowned for its rich history, vibrant culture, and beautiful architecture. The city is particularly famous for Mysore Palace, an imposing edifice with a striking combination of Hindu, Saracenic, and Rajput architectural styles.\n",
      "\n",
      "Mysuru also hosts the Mysore Dussehra festival, one of India's grandest and most famous cultural events celebrated every year. The city is known for its silk industry, producing some of the finest quality silk sarees in the world.\n",
      "\n",
      "Another notable attraction is the Chamundi Hills, where the Chamundeshwari Temple dedicated to the goddess Chamundeshwari, the patron deity of Mysuru, stands atop. The city is also home to several educational institutions of national importance such as the University of Mysore and the Indian Institute of Science Education and Research (IISER).\n",
      "\n",
      "Mysuru's pleasant climate, green surroundings, and rich cultural heritage make it a popular tourist destination in India.\n",
      " Mysuru is located in the southern part of India, in the state of Karnataka. It lies approximately 145 kilometers (90 miles) southwest of Bangalore, which is the capital city of Karnataka. Mysuru is situated at an elevation of about 768 meters (2,520 feet) above sea level in a region known as the Mysore Plateau or the Deccan Plateau. The city's geographical coordinates are:\n",
      "\n",
      "Latitude: 12.3958Â° N\n",
      "Longitude: 76.5426Â° E\n",
      "\n",
      "These coordinates can be used to find the exact location of Mysuru on maps and navigation apps.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me something about Mysuru.\"}\n",
    "]\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='mistral',\n",
    "    messages=messages\n",
    ")\n",
    "print(response['message']['content'])\n",
    "\n",
    "# Memory is retained by sending back the full messages list!\n",
    "messages.append(response['message'])\n",
    "\n",
    "# Follow-up query\n",
    "messages.append({\"role\": \"user\", \"content\": \"Where is that located?\"})\n",
    "response = ollama.chat(model='mistral', messages=messages)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "47c13882",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "openAI Equivalent\n",
    "client.chat.completions.create(\n",
    "  model=\"gpt-4.1-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a historian.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain Mysuru Palace\"}\n",
    "  ]\n",
    ")\n",
    "Hugging Face Equivalent\n",
    "from huggingface_hub import InferenceClient\n",
    "client = InferenceClient(model=\"mistralai/Mistral-7B-Instruct\")\n",
    "\n",
    "client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a historian.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain Mysuru Palace\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d93af4",
   "metadata": {},
   "source": [
    "# âœ… Ollama Python API â€” Two Different Interfaces\n",
    "\n",
    "Ollama provides two ways to call models:\n",
    "\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "1ï¸âƒ£ Simple API (Top-Level Functions)\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "âœ… Best for: Demos, learning, testing\n",
    "âœ… Automatic connection to localhost\n",
    "âœ… Minimal code\n",
    "\n",
    "    import ollama\n",
    "    response = ollama.generate(model=\"mistral\", prompt=\"Hello\")\n",
    "\n",
    "When to use:\n",
    "â€¢ Quick single-turn prompts\n",
    "â€¢ Exploring model responses\n",
    "â€¢ Teaching beginner LLM concepts\n",
    "\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "2ï¸âƒ£ Client API (Explicit Client)\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "âœ… Best for: Applications and production\n",
    "âœ… More control: host, retries, streaming, multi-turn chat\n",
    "\n",
    "    from ollama import Client\n",
    "    client = Client(host=\"http://localhost:11434\")\n",
    "    response = client.chat(model=\"mistral\", messages=[...])\n",
    "\n",
    "When to use:\n",
    "â€¢ Multi-turn chat apps\n",
    "â€¢ Remote Ollama servers (LAN / Docker)\n",
    "â€¢ Streaming token responses\n",
    "â€¢ Advanced settings and control\n",
    "\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ğŸ’¡ Summary:\n",
    "â€¢ Start learning with the simple API\n",
    "â€¢ Use Client API when building real chatbots / agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e77f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.generate(\n",
    "    model=\"mistral\",\n",
    "    prompt=\"Explain what embeddings are in 1 sentence.\"\n",
    ")\n",
    "\n",
    "print(response[\"response\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "037e737c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Who was the last king of Mysuru?\n",
      "Model:  The last Maharaja (King) of Mysore, as it is officially known today, was Sri Jayachamarajendra Wodeyar. He reigned from 1947 to 1949. However, it's important to note that the princely state of Mysore was integrated into the Indian Union in 1947 and Jayachamarajendra Wodeyar served as the Rajpramukh (Governor) of Mysore State from 1947 until 1952, when India became a republic and the position of Rajpramukh was abolished. He abdicated the throne in 1949.\n",
      "\n",
      "User: Where was he born?\n",
      "Model:  To provide an accurate answer, I would need a specific name or identity. If you're referring to a historical figure, public personality, or fictional character, please specify who you mean, and I'll be happy to help!\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "# Try multi-turn chat\n",
    "print(\"User: Who was the last king of Mysuru?\")\n",
    "res1 = ollama.generate(model=\"mistral\", prompt=\"Who was the last king of Mysuru?\")\n",
    "print(\"Model:\", res1[\"response\"])\n",
    "\n",
    "print(\"\\nUser: Where was he born?\")\n",
    "res2 = ollama.generate(model=\"mistral\", prompt=\"Where was he born?\")\n",
    "print(\"Model:\", res2[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464c23fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "\n",
    "client = Client()  # auto host = http://localhost:11434\n",
    "\n",
    "messages = []\n",
    "\n",
    "def chat(user_message):\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "    response = client.chat(model=\"mistral\", messages=messages)\n",
    "    messages.append(response[\"message\"])\n",
    "    print(\"Assistant:\", response[\"message\"][\"content\"])\n",
    "\n",
    "chat(\"Who was the last king of Mysuru?\")\n",
    "chat(\"Where was he born?\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c122ad43",
   "metadata": {},
   "source": [
    "LLMs do not have actual memory.\n",
    "We maintain memory by:\n",
    "â€¢ Storing conversation history in the application (Python list)\n",
    "â€¢ Sending full history again with every new request\n",
    "\n",
    "LLM memory is NOT inside the model â†’ It is in the APP layer.\n",
    "\n",
    "Later: LangChain helps automate this (Memory, Buffering, Summarization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5d8662",
   "metadata": {},
   "source": [
    "## 4. Tokens Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bb7393",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Who was the last king of Mysuru?\"\n",
    "response = ollama.generate(model=\"mistral\", \n",
    "                           prompt=f\"Tokenize this: {prompt}\")\n",
    "print(response['response'])\n",
    "\n",
    "\n",
    "# Tokens = cost + context length!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e63439",
   "metadata": {},
   "source": [
    "## 5. Embeddings (Meaning Understanding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f91ebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc6f7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.6645200613047445\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def embed(text):\n",
    "    return ollama.embeddings(model=\"mistral\", prompt=text)['embedding']\n",
    "\n",
    "v1 = embed(\"king\")\n",
    "v2 = embed(\"queen\")\n",
    "\n",
    "similarity = np.dot(v1, v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "print(\"Similarity:\", similarity)\n",
    "\n",
    "# HF Equivalent (commented)\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# sim = util.cos_sim(model.encode(\"king\"), model.encode(\"queen\"))\n",
    "# print(sim)\n",
    "print(v1)\n",
    "print(v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1632fcea",
   "metadata": {},
   "source": [
    "## 6. Temperature (Creativity Control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b281950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "q = \"Write a creative line about Mysuru.\"\n",
    "for t in [0.0, 0.7, 1.2]:\n",
    "    r = ollama.chat(model=\"mistral\",\n",
    "                    messages=[{\"role\":\"user\",\"content\":q}],\n",
    "                    options={'temperature':t})\n",
    "    print(f\"Temp {t}: {r['message']['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2477c467",
   "metadata": {},
   "source": [
    "## 7. Streaming Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c99a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = ollama.chat(\n",
    "    model=\"mistral\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me a short story about a tiger.\"}],\n",
    "    stream=True,\n",
    ")\n",
    "for chunk in stream:\n",
    "    print(chunk['message']['content'], end=\"\", flush=True)\n",
    "\n",
    "# OpenAI Equivalent\n",
    "# client.responses.create(..., stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6c30cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Transformers are a type of machine learning model used for natural language processing tasks, such as translating from one language to another or generating text. They were introduced by Google researchers in 2017 and have since become one of the most popular models for NLP tasks due to their ability to capture long-range dependencies in text.\n",
      "\n",
      "2. Transformers consist of a series of self-attention layers, which allow each word in the input sequence to attend to every other word in the sequence, weighted by the relevance of each word for understanding the current word's meaning. This allows the model to focus on relevant information and ignore irrelevant details, resulting in more accurate predictions.\n",
      "\n",
      "3. Unlike recurrent neural networks (RNNs) and convolutional neural networks (CNNs), transformers do not use recurrence or convolution, making them faster and easier to train on long sequences. Transformers also handle parallelization better than RNNs and CNNs, allowing for more efficient training on large datasets."
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "stream = client.generate(\n",
    "    model=\"mistral\",\n",
    "    prompt=\"Explain transformers in 3 short bullets.\",\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk[\"response\"], end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfe614a",
   "metadata": {},
   "source": [
    "## 8. Context Window Limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd11e4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Mysuru \" * 1500\n",
    "response = ollama.chat(\n",
    "    model=\"mistral\",\n",
    "    messages=[{\"role\":\"user\", \"content\":text + \"\\nWhat word did I repeat?\"}]\n",
    ")\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f152e0",
   "metadata": {},
   "source": [
    "## How Chat Memory Works\n",
    "We manually maintain full conversation in `messages` list\n",
    "\n",
    "â¡ Ollama does NOT store memory internally  \n",
    "â¡ We send all history every time (just like OpenAI)\n",
    "\n",
    "This leads to **context window limits** â€” motivation for RAG + LangChain Memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f61923",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Next: LangChain\n",
    "Before jumping, remember:\n",
    "| Feature | Raw Ollama | With LangChain |\n",
    "|--------|------------|----------------|\n",
    "| Memory | Manual messages[] | Builtâ€‘in memory modules |\n",
    "| Tools & Agents | Hard to implement | 1â€‘line setup |\n",
    "| RAG | Manual vector DB | Automated pipelines |\n",
    "| Workflows | Custom code | Chains & Executors |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

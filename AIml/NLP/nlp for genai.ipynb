{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7800bc-71a4-405d-b918-b1fce3a666fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = \"Urgent: Account issue needs resolution!\"\n",
    "pattern = r\"(urgent|issue)\"\n",
    "matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "print(\"Keywords found:\", matches)  # Output: ['Urgent', 'issue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd13ccdf-ac97-4016-956d-f37806792451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6da8ce3-c3c9-43a4-9a3b-5f2e1c33d625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"Bank service is great!\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)  # ['Bank', 'service', 'is', 'great', '!']\n",
    "\n",
    "# Alternative with spaCy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "tokens_spacy = [token.text for token in doc]\n",
    "print(\"spaCy Tokens:\", tokens_spacy)  # ['Bank', 'service', 'is', 'great', '!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18973c90-e58d-43a9-87f9-40849e044c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"running\", \"cancers\", \"university\"]\n",
    "stems = [stemmer.stem(word) for word in words]\n",
    "print(\"Stems:\", stems)  # ['run', 'cancer', 'univers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f4b5ef-76cc-402d-ac52-95299ae2eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [(\"better\", \"a\"), (\"ran\", \"v\")]\n",
    "lemmas = [lemmatizer.lemmatize(word, pos=pos) for word, pos in words]\n",
    "print(\"Lemmas:\", lemmas)  # ['good', 'run']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db00845e-d0cd-405a-804e-29cf6b5e9c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "text = \"The bank is great\"\n",
    "filtered = [word for word in text.lower().split() if word not in stop_words]\n",
    "print(\"Filtered:\", filtered)  # ['bank', 'great']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafcd183-2213-4b25-967e-bdcd2452dbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams, word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"Bank offers good service\"\n",
    "tokens = word_tokenize(text)\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "print(\"Bigrams:\", bigrams)  # [('Bank', 'offers'), ('offers', 'good'), ('good', 'service')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ab5686-2849-415a-a3d0-32128e077e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Lightweight, free\n",
    "text = \"Bank offers loans\"\n",
    "doc = nlp(text)\n",
    "pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "print(\"POS Tags:\", pos_tags)  # [('Bank', 'NOUN'), ('offers', 'VERB'), ('loans', 'NOUN')]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d1a590ed-32a0-4d26-a209-ab33de5ea707",
   "metadata": {},
   "source": [
    "Next steps: Try dependency parsing (token.dep_) to see sentence structure, or process longer texts. \n",
    ": Change the sentence to \"The bank offers loans\" and see how articles (\"The\") tag as DET (determiner)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19e326e-188c-4c71-9ed7-6d81c44a2df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Bank offers loans\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text} --> {token.dep_} --> {token.head.text}\")\n",
    "# Output: Bank --> nsubj --> offers\n",
    "#         offers --> ROOT --> offers\n",
    "#         loans --> dobj --> offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c922bfef-37e3-407b-a1c4-5bd35fcc646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"HDFC Bank launches new loan in Mumbai on 2025-05-10\")\n",
    "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "print(\"Entities:\", entities)  # [('HDFC Bank', 'ORG'), ('Mumbai', 'GPE'), ('2025-05-10', 'DATE')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8960c92d-8a9d-4644-b1c8-a50fcd2d2ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "texts = [\"Bank is great\", \"but bank service is poor\"]\n",
    "vectorizer = CountVectorizer()\n",
    "bow = vectorizer.fit_transform(texts)\n",
    "print(\"BoW Matrix:\", bow.toarray())\n",
    "print(\"Vocabulary:\", vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caca894-3a2d-450f-be6e-6e8b5a38bdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "texts = [\"Bank has fraud issues\", \"Service is great\"]\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform(texts)\n",
    "print(\"TF-IDF Matrix:\", tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7df946-2bc4-4962-88a3-e310d3778183",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "matrix = [[1, 0], [0, 1]]  # Simplified term-document matrix\n",
    "svd = TruncatedSVD(n_components=1)\n",
    "lsa = svd.fit_transform(matrix)\n",
    "print(\"LSA:\", lsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43db750e-6927-4a71-a71e-00d40df37887",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250587e9-e8cd-4e5a-92a3-7947fc8cd6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd2d7b9-c169-43de-b2bf-e98f99ff06ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f572b25-06b8-48bd-9364-d6260e85601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b890d0fd-f036-43b5-8cd6-e65334a54c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "#!pip install gensim nltk\n",
    "# Import libraries\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Sample text data (e.g., poker logs or BFSI notes)\n",
    "documents = [\n",
    "    \"Player P1 bet 1000 on flush, won game\",\n",
    "    \"Player P2 raised 5000, lost due to fold\",\n",
    "    \"Player P3 bet 20000 on straight, high risk\",\n",
    "    \"KYC document requires ID, income proof\",\n",
    "]\n",
    "\n",
    "# Preprocess text\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "processed_docs = [preprocess(doc) for doc in documents]\n",
    "\n",
    "# LDA: Create dictionary and corpus\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "# Train LDA model\n",
    "lda_model = LdaModel(corpus, num_topics=2, id2word=dictionary, passes=10)\n",
    "print(\"LDA Topics:\")\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic {idx}: {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4f0a76-b31a-45a3-9a5f-c9809455a366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIPGRAM --> predicts context from words --> good for rare words --> antibiotics , resistance\n",
    "# CROSSBOW --> predicts words from context --> faster for common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71fa3d3-b690-4247-9866-b94b5c5024bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec: Train model\n",
    "w2v_model = Word2Vec(sentences=processed_docs, vector_size=100, window=5, min_count=1, workers=4, )\n",
    "print(\"\\nWord2Vec Example:\")\n",
    "word = \"kyc\"\n",
    "similar_words = w2v_model.wv.most_similar(word, topn=3)\n",
    "print(f\"Words similar to '{word}': {similar_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59357d32-51fd-404a-b818-62ba9d97ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "texts = [[\"bank\", \"loan\"], [\"service\", \"great\"]]\n",
    "dictionary = Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "lda = LdaModel(corpus, num_topics=2, id2word=dictionary)\n",
    "print(\"Topics:\", lda.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33010f1c-dbb0-4fc0-b800-09a0514df2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [\n",
    "    [\"bank\", \"offers\", \"loan\"],\n",
    "    [\"bank\", \"provides\", \"credit\"],\n",
    "    [\"service\", \"is\", \"great\"]\n",
    "]\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "print(\"Vector for 'loan':\", model.wv[\"loan\"][:5])  # First 5 dimensions\n",
    "print(\"Similar to 'loan':\", model.wv.most_similar(\"loan\", topn=2))  # ['credit', 'bank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ba4998-c431-4048-a593-d79bcf6f08f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Simplified: Load pre-trained GloVe\n",
    "glove_vectors = {\"bank\": np.random.rand(100), \"loan\": np.random.rand(100)}\n",
    "print(\"GloVe vector for 'bank':\", glove_vectors[\"bank\"][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0309f3d9-aaf1-485e-aa8b-fd08961acd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "sentences = [[\"antibiotic\", \"resistance\"], [\"drug\", \"therapy\"]]\n",
    "model = FastText(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "print(\"Vector for 'antibiotic':\", model.wv[\"antibiotic\"][:5])\n",
    "print(\"Similar to 'antibiotic':\", model.wv.most_similar(\"antibiotic\", topn=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff64bfa-4504-4626-a0d6-c919f2e7b8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "rnn = nn.RNN(input_size=10, hidden_size=20, batch_first=True)\n",
    "input_seq = torch.randn(1, 5, 10)  # Batch, seq len, input size\n",
    "output, hn = rnn(input_seq)\n",
    "fc = nn.Linear(20, 2)\n",
    "final_output = fc(output[:, -1, :])  # Last time step\n",
    "print(\"RNN Output:\", final_output.shape)  # [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7ce5bc-f6c6-4a24-a474-e4859a0d9e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(input_size=10, hidden_size=20, batch_first=True)\n",
    "input_seq = torch.randn(1, 5, 10)\n",
    "output, (hn, cn) = lstm(input_seq)\n",
    "fc = nn.Linear(20, 2)\n",
    "final_output = fc(output[:, -1, :])\n",
    "print(\"LSTM Output:\", final_output.shape)  # [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7d18ba-0160-40ff-943b-a36e8ea9a274",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = nn.GRU(input_size=10, hidden_size=20, batch_first=True)\n",
    "input_seq = torch.randn(1, 5, 10)\n",
    "output, hn = gru(input_seq)\n",
    "fc = nn.Linear(20, 2)\n",
    "final_output = fc(output[:, -1, :])\n",
    "print(\"GRU Output:\", final_output.shape)  # [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78b2b2c-4a8e-49aa-b38c-d22fe5ea127b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83807c4c-b37d-42b3-aca4-a981e98cfdb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
